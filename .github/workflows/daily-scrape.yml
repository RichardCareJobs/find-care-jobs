name: Daily Job Scrape
on:
  schedule:
    - cron: 0 0 * * *
  workflow_dispatch: null
jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
          persist-credentials: true
      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: "18"
      - name: Install dependencies
        run: npm install
      - name: Run daily scrape
        run: node scrape/scrape.js
      - name: List all CSVs
        run: |
          echo "Workspace at $(pwd)"
          find "$(pwd)" -type f -name '*.csv' -exec echo "  â†’ {}" ';'
      - name: "Debug: show CSV head"
        run: |
          echo "--- job_listings.csv head ---"
          head -n 5 scrape/job_listings.csv || echo "(missing)"
          echo "--- sponsored_jobs.csv head ---"
          head -n 5 scrape/sponsored_jobs.csv || echo "(missing)"
      - name: Archive CSVs
        run: >
          TIMESTAMP=$(date -u +'%Y-%m-%d_%H%M')

          mkdir -p archive

          cp scrape/job_listings.csv  archive/job_listings_${TIMESTAMP}.csv

          cp scrape/sponsored_jobs.csv archive/sponsored_jobs_${TIMESTAMP}.csv || echo "no sponsored file"
      - name: Commit updated + archived CSVs
        uses: EndBug/add-and-commit@v9
        with:
          add: "scrape/job_listings.csv scrape/sponsored_jobs.csv archive/*.csv"
          commit: "--allow-empty"      # optional, to force a commit even if nothing changed
          message: "chore: automated daily scrape & archive"

